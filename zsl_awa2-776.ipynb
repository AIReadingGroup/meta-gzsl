{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6629\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.cuda.set_device(3)\n",
    "\n",
    "seed=6629 #torch.randint(1,10000,(1,))\n",
    "print(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init dataset\n",
      "train_features.shape: (23527, 2048)\n",
      "train_label.shape:  (23527, 1)\n",
      "all_attributes.shape:  (50, 85)\n",
      "test_features.shape:  (7913, 2048)\n",
      "test_label.shape:  (7913, 1)\n",
      "testclasses_id.shape:  (10,)\n",
      "test_attributes.shape:  torch.Size([10, 85])\n",
      "test_seen_features.shape:  torch.Size([5882, 2048])\n"
     ]
    }
   ],
   "source": [
    "# step 1: init dataset\n",
    "print(\"init dataset\")\n",
    "\n",
    "dataroot = '/data/dataset/zsl/data'\n",
    "dataset = 'AwA2_data'\n",
    "image_embedding = 'res101' \n",
    "class_embedding = 'att_splits'\n",
    "\n",
    "\n",
    "matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + image_embedding + \".mat\")\n",
    "feature = matcontent['features'].T\n",
    "label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + class_embedding + \".mat\")\n",
    "# numpy array index starts from 0, matlab starts from 1\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "\n",
    "attribute = 1*matcontent['original_att'].T \n",
    "\n",
    "x = feature[trainval_loc] # train_features\n",
    "train_label = label[trainval_loc].astype(int)  # train_label\n",
    "att = attribute[train_label] # train attributes\n",
    "\n",
    "x_test = feature[test_unseen_loc]  # test_feature\n",
    "test_label = label[test_unseen_loc].astype(int) # test_label\n",
    "x_test_seen = feature[test_seen_loc]  #test_seen_feature\n",
    "test_label_seen = label[test_seen_loc].astype(int) # test_seen_label\n",
    "test_id = np.unique(test_label)   # test_id\n",
    "att_pro = attribute[test_id]      # test_attribute\n",
    "\n",
    "\n",
    "# train set\n",
    "#train_features=torch.from_numpy(x)\n",
    "train_features=x\n",
    "print('train_features.shape: ' + str(train_features.shape))\n",
    "\n",
    "train_label=np.array(torch.from_numpy(train_label).unsqueeze(1))\n",
    "#train_label=torch.from_numpy(train_label).unsqueeze(1)\n",
    "print('train_label.shape:  '+str(train_label.shape))\n",
    "\n",
    "# attributes\n",
    "all_attributes=np.array(attribute)\n",
    "print('all_attributes.shape:  '+str(all_attributes.shape))\n",
    "\n",
    "attributes = torch.from_numpy(attribute)\n",
    "# test set\n",
    "\n",
    "# test_features=torch.from_numpy(x_test)\n",
    "test_features=x_test\n",
    "print('test_features.shape:  '+ str(test_features.shape))\n",
    "\n",
    "test_label=np.array(torch.from_numpy(test_label).unsqueeze(1))\n",
    "print('test_label.shape:  ' +str(test_label.shape))\n",
    "\n",
    "testclasses_id = np.array(test_id)\n",
    "print('testclasses_id.shape:  ' +str(testclasses_id.shape))\n",
    "\n",
    "test_attributes = torch.from_numpy(att_pro).float()\n",
    "print('test_attributes.shape:  ' +str(test_attributes.shape))\n",
    "\n",
    "\n",
    "test_seen_features = torch.from_numpy(x_test_seen)\n",
    "print('test_seen_features.shape:  ' +str(test_seen_features.shape))\n",
    "\n",
    "test_seen_label = torch.from_numpy(test_label_seen)\n",
    "\n",
    "# train_features=np.concatenate([train_features,np.array(test_seen_features)],0)\n",
    "# train_label=np.concatenate([train_label,np.reshape(np.array(test_seen_label),(test_seen_label.shape[0],1))])\n",
    "\n",
    "train_data = [train_features,train_label]\n",
    "test_data = [test_features,test_label]\n",
    "\n",
    "unq_train_labels=np.unique(train_label)\n",
    "unq_test_labels=np.unique(test_label)\n",
    "#train_data = TensorDataset(train_features,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.  , -1.  , -1.  , ...,  2.35,  9.7 ,  8.38],\n",
       "       [39.25,  1.39,  0.  , ..., 58.64, 20.14, 11.39],\n",
       "       [83.4 , 64.79,  0.  , ..., 15.77, 13.41, 15.42],\n",
       "       ...,\n",
       "       [63.57, 43.1 ,  0.  , ..., 35.95, 28.26,  5.  ],\n",
       "       [55.31, 55.46,  0.  , ...,  5.04, 18.89, 72.99],\n",
       "       [10.22, 21.53, 27.73, ...,  3.96, 14.05, 37.98]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_train_features=train_features\n",
    "# orig_train_label=train_label\n",
    "\n",
    "# cubNshot=10\n",
    "# Ncls=unq_train_labels.shape[0]\n",
    "# few_train_features=[]\n",
    "# few_train_label=[]\n",
    "    \n",
    "# for cur_class in unq_train_labels:\n",
    "#     indx=np.where(train_label==cur_class)[0]\n",
    "#     selected_img = np.random.choice(indx, cubNshot, False)\n",
    "#     few_train_features.append(train_features[selected_img])\n",
    "#     few_train_label.append(train_label[selected_img])\n",
    "    \n",
    "# perm = np.random.permutation(Ncls * cubNshot)\n",
    "# few_train_features = np.array(few_train_features).reshape(Ncls * cubNshot, 2048)[perm]\n",
    "# few_train_label = np.array(few_train_label).reshape(Ncls * cubNshot,1)[perm]\n",
    "\n",
    "# train_features=few_train_features\n",
    "# train_label=few_train_label\n",
    "\n",
    "# print('train_features.shape: ' + str(train_features.shape))\n",
    "# print('train_label.shape:  '+str(train_label.shape))\n",
    "\n",
    "# train_data = [train_features,train_label]\n",
    "# test_data = [test_features,test_label]\n",
    "\n",
    "# print('train_data.shape: ' + str(train_data.shape))\n",
    "# print('test_data.shape:  '+str(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.modelG = nn.Sequential(\n",
    "            nn.Linear(args.attri_dim+args.noise_dim, 2048),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048, 0.8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048, 0.8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, args.input_shape)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, noise, attri,weightsM=None):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((attri,noise), -1)\n",
    "        if weightsM is None:\n",
    "            img = self.modelG(gen_input)\n",
    "        else:\n",
    "            i=0\n",
    "            weights=weightsM[0]\n",
    "            for m in self.modelG.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "                if isinstance(m,nn.BatchNorm1d):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "            img = self.modelG(gen_input)\n",
    "        \n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.modelD = nn.Sequential(\n",
    "            nn.Linear(args.input_shape+args.attri_dim, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, attri,weightsM=None):\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), attri), -1)\n",
    "        if weightsM is None:\n",
    "            real_fake = self.modelD(d_in)\n",
    "        else:\n",
    "            i=0\n",
    "            weights=weightsM[1]\n",
    "            for m in self.modelD.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "                if isinstance(m,nn.BatchNorm1d):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "            real_fake = self.modelD(d_in)\n",
    "        \n",
    "        return real_fake\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.modelC = nn.Sequential(\n",
    "            nn.Linear(args.input_shape, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 512),\n",
    "            #nn.BatchNorm1d(512, 0.8),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, args.num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_gen, target,weightsM=None):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        if weightsM is None:\n",
    "            output = self.modelC(img_gen)\n",
    "        else:\n",
    "            i=0\n",
    "            weights=weightsM[2]\n",
    "            for m in self.modelC.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "                if isinstance(m,nn.BatchNorm1d):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "            output = self.modelC(img_gen)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (modelC): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class all_arguments():\n",
    "    n_way=10\n",
    "    k_spt=5\n",
    "    k_qry=3\n",
    "    \n",
    "    imgsz=2048\n",
    "    sigma_ts=1\n",
    "    sigma_tr=0.1\n",
    "    \n",
    "    meta_lr=1e-5\n",
    "    meta_lrD=1e-3\n",
    "    update_lr=1e-3\n",
    "    update_step=5\n",
    "\n",
    "    input_shape=2048\n",
    "    num_class=50\n",
    "    attri_dim=85\n",
    "    noise_dim=40\n",
    "    clssifier_weight=0.05\n",
    "\n",
    "    # Get options\n",
    "    attributes=attributes\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "    \n",
    "args=all_arguments()\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "classifier=Classifier()\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    classifier.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "    \n",
    "para=list(classifier.parameters())\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "from    torch.utils.data import TensorDataset, DataLoader\n",
    "from    torch import optim\n",
    "import  numpy as np\n",
    "\n",
    "# [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "glen=len(list(p for p in generator.parameters() if p.requires_grad))\n",
    "gen_para=list(p for p in generator.parameters() if p.requires_grad)+ list(p for p in classifier.parameters() if p.requires_grad)\n",
    "disc_para=[p for p in discriminator.parameters() if p.requires_grad]\n",
    "disc_optim = optim.SGD(disc_para, lr=args.meta_lrD,weight_decay=1e-6)\n",
    "gen_optim = optim.Adam(gen_para, lr=args.meta_lr,betas=(0.9, 0.999),weight_decay=1e-6)\n",
    "\n",
    "disc_schedular = StepLR(disc_optim,step_size=100,gamma=0.95)\n",
    "gen_schedular = StepLR(gen_optim,step_size=100,gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = args.update_lr\n",
    "        self.meta_lr = args.meta_lr\n",
    "        self.n_way = args.n_way\n",
    "        self.k_spt = args.k_spt\n",
    "        self.k_qry = args.k_qry\n",
    "        self.cls_weight=args.clssifier_weight\n",
    "        self.update_step = args.update_step\n",
    "        cuda=args.cuda\n",
    "        self.noise_dim=args.noise_dim\n",
    "        #self.all_loss=all_loss()\n",
    "        \n",
    "        self.FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "        self.glen=glen\n",
    "        self.gen_para=gen_para\n",
    "        self.disc_optim = disc_optim\n",
    "        self.gen_optim = gen_optim\n",
    "\n",
    "        self.disc_schedular = disc_schedular\n",
    "        self.gen_schedular = gen_schedular\n",
    "\n",
    "    def clip_grad_by_norm_(self, grad, max_norm):\n",
    "        \"\"\"\n",
    "        in-place gradient clipping.\n",
    "        :param grad: list of gradients\n",
    "        :param max_norm: maximum norm allowable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        total_norm = 0\n",
    "        counter = 0\n",
    "        for g in grad:\n",
    "            param_norm = g.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            counter += 1\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            for g in grad:\n",
    "                g.data.mul_(clip_coef)\n",
    "\n",
    "        return total_norm/counter\n",
    "    \n",
    "    def all_loss(self, img_feature,img_labels,fast_weight=None):\n",
    "        batch_size = img_feature.shape[0]\n",
    "        FloatTensor=self.FloatTensor\n",
    "        LongTensor=self.LongTensor\n",
    "        img_labels=img_labels.type(LongTensor)\n",
    "\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(img_feature.type(FloatTensor))\n",
    "        gen_attri = Variable(attributes[img_labels].type(FloatTensor))\n",
    "        \n",
    "        z = Variable(FloatTensor(np.random.normal(0,args.sigma_tr, (batch_size, self.noise_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_attri,fast_weight)\n",
    "        validity = discriminator(gen_imgs, gen_attri,fast_weight)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        #print('g_loss: '+str(g_loss))\n",
    "        \n",
    "        \n",
    "        validity_real = discriminator(real_imgs, gen_attri,fast_weight)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "        #print('d_real_loss: '+str(d_real_loss))\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_attri,fast_weight)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "        #print('d_fake_loss: '+str(d_fake_loss))\n",
    "\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        \n",
    "        cls_out=classifier(gen_imgs,img_labels,fast_weight)\n",
    "        cls_out=F.log_softmax(cls_out, dim=1)\n",
    "        c_loss = F.nll_loss(cls_out, img_labels)\n",
    "        \n",
    "        \n",
    "        return g_loss, d_loss, c_loss, cls_out\n",
    "    \n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "        :param x_spt:   [b, setsz, d]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, d]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        task_num, setsz,d = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        losses_gen = [0 for _ in range(self.update_step + 1)]  # losses_q[i], i is tasks idx\n",
    "        losses_dis = [0 for _ in range(self.update_step + 1)]\n",
    "        losses_cla = [0 for _ in range(self.update_step + 1)]\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i])\n",
    "             \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(y_spt[i].view_as(pred)).sum().item()\n",
    "            \n",
    "            \n",
    "            joint_gen_loss=g_loss+self.cls_weight*c_loss\n",
    "                                      \n",
    "            gen_grad = torch.autograd.grad(joint_gen_loss, self.gen_para)\n",
    "            fast_weights_G = list(map(lambda g: g[1] - self.update_lr * g[0], zip(gen_grad, self.gen_para)))\n",
    "            \n",
    "            disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "            fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "\n",
    "#             clas_grad = torch.autograd.grad(c_loss, self.discriminator.parameters())\n",
    "#             fast_weights_C = list(map(lambda c: c[1] - self.update_lr * c[0], zip(clas_grad, self.classifier.parameters())))\n",
    "            fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i])\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                losses_gen[0] += g_loss\n",
    "                losses_dis[0] += d_loss\n",
    "                losses_cla[0] += c_loss\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # how to initialise with the fast weight\n",
    "                losses_gen[1] += g_loss\n",
    "                losses_dis[1] += d_loss\n",
    "                losses_cla[1] += c_loss\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i],fast_weight)\n",
    "                joint_gen_loss=g_loss+self.cls_weight*c_loss\n",
    "\n",
    "                gen_grad = torch.autograd.grad(joint_gen_loss, self.gen_para)\n",
    "                fast_weights_G = list(map(lambda g: g[1] - self.update_lr * g[0], zip(gen_grad, self.gen_para)))\n",
    "\n",
    "                disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "                fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "                \n",
    "                fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "                # 2. compute grad on theta_pi\n",
    "                \n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step. \n",
    "                losses_gen[k+1] += g_loss\n",
    "                losses_dis[k+1] += d_loss\n",
    "                losses_cla[k+1] += c_loss\n",
    "                corrects[k+1] = corrects[k+1] + correct\n",
    "                \n",
    "                ################################## Unseen class test #####################################\n",
    "\n",
    "            # 4. record last step's loss for task i\n",
    "            losses_gen.append(g_loss)\n",
    "            losses_dis.append(d_loss)\n",
    "            losses_cla.append(c_loss)\n",
    "                                      \n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_gen=0\n",
    "        for itr in range(task_num):\n",
    "            loss_gen+=losses_gen[-task_num:][itr]\n",
    "        loss_gen=loss_gen/task_num\n",
    "        \n",
    "        loss_dis=0\n",
    "        for itr in range(task_num):\n",
    "            loss_dis+=losses_dis[-task_num:][itr]\n",
    "        loss_dis=loss_dis/task_num\n",
    "        \n",
    "        loss_cla=0\n",
    "        for itr in range(task_num):\n",
    "            loss_cla+=losses_cla[-task_num:][itr]\n",
    "        loss_cla=loss_cla/task_num\n",
    "            \n",
    "        joint_Gloss=loss_gen+self.cls_weight*loss_cla\n",
    "        \n",
    "        # optimize theta parameters\n",
    "        self.disc_optim.zero_grad()\n",
    "        self.gen_optim.zero_grad()\n",
    "        \n",
    "        joint_Gloss.backward()\n",
    "        loss_dis.backward()\n",
    "        \n",
    "        self.disc_optim.step()\n",
    "        self.gen_optim.step()\n",
    "        \n",
    "        \n",
    "        ######################################################################################\n",
    "        losses_gen = [0 for _ in range(self.update_step + 1)]  # losses_q[i], i is tasks idx\n",
    "        losses_dis = [0 for _ in range(self.update_step + 1)]\n",
    "        losses_cla = [0 for _ in range(self.update_step + 1)]\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i])\n",
    "             \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(y_spt[i].view_as(pred)).sum().item()\n",
    "            \n",
    "            disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "            fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "\n",
    "#             clas_grad = torch.autograd.grad(c_loss, self.discriminator.parameters())\n",
    "#             fast_weights_C = list(map(lambda c: c[1] - self.update_lr * c[0], zip(clas_grad, self.classifier.parameters())))\n",
    "            fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i])\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                losses_gen[0] += g_loss\n",
    "                losses_dis[0] += d_loss\n",
    "                losses_cla[0] += c_loss\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # how to initialise with the fast weight\n",
    "                losses_gen[1] += g_loss\n",
    "                losses_dis[1] += d_loss\n",
    "                losses_cla[1] += c_loss\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i],fast_weight)\n",
    "\n",
    "                disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "                fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "                \n",
    "                fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "                # 2. compute grad on theta_pi\n",
    "                \n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step. \n",
    "                losses_gen[k+1] += g_loss\n",
    "                losses_dis[k+1] += d_loss\n",
    "                losses_cla[k+1] += c_loss\n",
    "                corrects[k+1] = corrects[k+1] + correct\n",
    "                \n",
    "                ################################## Unseen class test #####################################\n",
    "\n",
    "            # 4. record last step's loss for task i\n",
    "            losses_gen.append(g_loss)\n",
    "            losses_dis.append(d_loss)\n",
    "            losses_cla.append(c_loss)\n",
    "                                      \n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        \n",
    "        loss_dis=0\n",
    "        for itr in range(task_num):\n",
    "            loss_dis+=losses_dis[-task_num:][itr]\n",
    "        loss_dis=loss_dis/task_num\n",
    "        \n",
    "        # optimize theta parameters\n",
    "        self.disc_optim.zero_grad()\n",
    "        loss_dis.backward()\n",
    "        \n",
    "        self.disc_optim.step()\n",
    "        ######################################################################################\n",
    "         \n",
    "        \n",
    "        accs = np.array(corrects) / (querysz * task_num)\n",
    "        #all_loss=[losses_gen,losses_dis,losses_cla]\n",
    "\n",
    "        return accs, loss_gen,loss_dis, loss_cla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torchvision.transforms as transforms\n",
    "from    PIL import Image\n",
    "import  os.path\n",
    "import  numpy as np\n",
    "\n",
    "\n",
    "class zsl_NShot:\n",
    "\n",
    "    def __init__(self, trainData,testData, batchsz, n_way, k_shot, k_query):\n",
    "        \"\"\"\n",
    "        Different from mnistNShot, the\n",
    "        :param root:\n",
    "        :param batchsz: task num\n",
    "        :param n_way:\n",
    "        :param k_shot:\n",
    "        :param k_qry:\n",
    "        :param imgsz:\n",
    "        \"\"\"\n",
    "        self.x_train, self.x_test = trainData, testData  \n",
    "        # self.normalization()\n",
    "        self.batchsz = batchsz # number of task\n",
    "        self.n_cls = 50  # \n",
    "        self.n_way = n_way  # n way\n",
    "        self.k_shot = k_shot  # k shot\n",
    "        self.k_query = k_query  # k query\n",
    "\n",
    "        # save pointer of current read batch in total cache\n",
    "        self.indexes = {\"train\": 0, \"test\": 0}\n",
    "        self.datasets = {\"train\": self.x_train, \"test\": self.x_test}  # original data cached\n",
    "        #print(\"DB: train\", self.x_train[0].shape, \"test\", self.x_test[0].shape)\n",
    "\n",
    "        self.datasets_cache = {\"train\": self.load_data_cache(self.datasets[\"train\"])} # current epoch data cached\n",
    "                              # \"test\": self.load_data_cache(self.datasets[\"test\"])}\n",
    "\n",
    "    def normalization(self):\n",
    "        \"\"\"\n",
    "        Normalizes our data, to have a mean of 0 and sdt of 1\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "        # print(\"before norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "        self.x_train = (self.x_train - self.mean) / self.std\n",
    "        self.x_test = (self.x_test - self.mean) / self.std\n",
    "\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "\n",
    "    # print(\"after norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "\n",
    "    def load_data_cache(self, data_pack):\n",
    "        \"\"\"\n",
    "        Collects several batches data for N-shot learning\n",
    "        :param data_pack: [N,2048]\n",
    "        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "        \"\"\"\n",
    "        #  take 5 way 1 shot as example: 5 * 1\n",
    "        setsz = self.k_shot * self.n_way\n",
    "        querysz = self.k_query * self.n_way\n",
    "        data_cache = []\n",
    "\n",
    "        unq_labels=np.unique(data_pack[1])\n",
    "        labels=np.array(data_pack[1])\n",
    "        \n",
    "        Data=data_pack[0]\n",
    "       \n",
    "        x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
    "        for i in range(self.batchsz):  # one batch means one set\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
    "#             print('unq labels shape: '+str(unq_labels.shape)+'  ' +str(i))\n",
    "            selected_cls = np.random.choice(unq_labels, 2*self.n_way, False)\n",
    "#             print(unq_labels)\n",
    "#                 print('selected: '+str(selected_cls))\n",
    "\n",
    "            for j, cur_class in enumerate(selected_cls[:self.n_way]):\n",
    "                indx=np.where(labels==cur_class)[0]\n",
    "                selected_img = np.random.choice(indx, self.k_shot, False)\n",
    "                x_spt.append(Data[selected_img])\n",
    "                y_spt.append(labels[selected_img])\n",
    "                #print('shape: '+str(np.array(x_spt).shape))\n",
    "\n",
    "            for j, cur_class in enumerate(selected_cls[self.n_way:]):\n",
    "                indx=np.where(labels==cur_class)[0]\n",
    "                selected_img = np.random.choice(indx, self.k_query, False)\n",
    "                x_qry.append(Data[selected_img])\n",
    "                y_qry.append(labels[selected_img])\n",
    "                #print('shape: '+str(np.array(x_spt).shape))\n",
    "\n",
    "            # shuffle inside a batch\n",
    "            perm = np.random.permutation(self.n_way * self.k_shot)\n",
    "            x_spt = np.array(x_spt).reshape(self.n_way * self.k_shot, 2048)[perm]\n",
    "            y_spt = np.array(y_spt).reshape(self.n_way * self.k_shot)[perm]\n",
    "            perm = np.random.permutation(self.n_way * self.k_query)\n",
    "            x_qry = np.array(x_qry).reshape(self.n_way * self.k_query, 2048)[perm]\n",
    "            y_qry = np.array(y_qry).reshape(self.n_way * self.k_query)[perm]\n",
    "\n",
    "            # append [N,2048] => [b, N,2048]\n",
    "            x_spts.append(x_spt)\n",
    "            y_spts.append(y_spt)\n",
    "            x_qrys.append(x_qry)\n",
    "            y_qrys.append(y_qry)\n",
    "\n",
    "\n",
    "        # [b, N,2048]\n",
    "        x_spts = np.array(x_spts).astype(np.float32).reshape(self.batchsz, setsz, 2048)\n",
    "        y_spts = np.array(y_spts).astype(np.int).reshape(self.batchsz, setsz)\n",
    "        # [b, N,2048]\n",
    "        x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 2048)\n",
    "        y_qrys = np.array(y_qrys).astype(np.int).reshape(self.batchsz, querysz)\n",
    "\n",
    "        data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "\n",
    "        return data_cache\n",
    "\n",
    "    def next(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Gets next batch from the dataset with name.\n",
    "        :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # update cache if indexes is larger cached num\n",
    "        if self.indexes[mode] >= len(self.datasets_cache[mode]):\n",
    "            self.indexes[mode] = 0\n",
    "            self.datasets_cache[mode] = self.load_data_cache(self.datasets[mode])\n",
    "\n",
    "        next_batch = self.datasets_cache[mode][self.indexes[mode]]\n",
    "        self.indexes[mode] += 1\n",
    "\n",
    "        return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "def test_zsl(genunseen_input,test_labels_repeat,test_features,testLabels):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            gen_imgs = generator(genunseen_input[:,attribute.shape[1]:], genunseen_input[:,:attribute.shape[1]])\n",
    "            \n",
    "            pseudoTrainData = normalize(gen_imgs.cpu() , axis =1)\n",
    "            testData = normalize(test_features , axis=1)\n",
    "            pseudoTrainLabels=test_labels_repeat\n",
    "            testLabels=testLabels\n",
    "\n",
    "            clf5 = svm.SVC(C=1,kernel='linear',class_weight='balanced')\n",
    "\n",
    "            clf5.fit(pseudoTrainData, pseudoTrainLabels)\n",
    "            #print 'Predicting...'\n",
    "            pred = clf5.predict(testData)\n",
    "            unseen_accuracy=accuracy_score(testLabels , pred)\n",
    "\n",
    "        return unseen_accuracy\n",
    "    \n",
    "Nsamples=100\n",
    "def test():\n",
    "    testunq_labels=np.unique(test_label)\n",
    "    lab_attribute=np.reshape(attribute[testunq_labels[0]],[1,attribute.shape[1]])\n",
    "    test_attri_repeat=[]\n",
    "    test_labels_repeat=[]\n",
    "    for i in testunq_labels:\n",
    "        lab_attribute=np.reshape(attribute[i],[1,attribute.shape[1]])\n",
    "        test_attri_repeat.append(np.repeat(lab_attribute,Nsamples,axis=0))\n",
    "        test_labels_repeat.append(np.repeat(i,Nsamples,axis=0))\n",
    "    test_attri_repeat=np.concatenate(test_attri_repeat,0)\n",
    "#     z1 = np.random.normal(0, 0.5, (test_attri_repeat.shape))\n",
    "#     test_attri_repeat=test_attri_repeat+z1\n",
    "    \n",
    "    test_labels_repeat=np.concatenate(test_labels_repeat,0)\n",
    "    z = np.random.normal(0, args.sigma_ts, (Nsamples*testunq_labels.shape[0], args.noise_dim))\n",
    "    genunseen_input = np.concatenate((test_attri_repeat,z), 1)\n",
    "\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    genunseen_input=(torch.from_numpy(genunseen_input)).type(FloatTensor)\n",
    "\n",
    "\n",
    "    unseen_accuracy=test_zsl(genunseen_input,test_labels_repeat,test_features,test_label)\n",
    "    return unseen_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  torch.Size([10, 50, 2048])\n",
      "train_labels :  torch.Size([10, 50])\n",
      "test :  torch.Size([10, 30, 2048])\n",
      "test_labels :  torch.Size([10, 30])\n",
      "5000-Accuracy: 0.987::::Loss::::0.575 : 0.138 : 0.066 Accuracy: 0.675 Best : 0.675\n",
      "5100-Accuracy: 0.988::::Loss::::0.636 : 0.132 : 0.052 Accuracy: 0.742 Best : 0.742\n",
      "5200-Accuracy: 0.988::::Loss::::0.518 : 0.145 : 0.052 Accuracy: 0.712 Best : 0.742\n",
      "5300-Accuracy: 0.982::::Loss::::0.56 : 0.133 : 0.074 Accuracy: 0.659 Best : 0.742\n",
      "5400-Accuracy: 0.989::::Loss::::0.623 : 0.135 : 0.042 Accuracy: 0.691 Best : 0.742\n",
      "5500-Accuracy: 0.987::::Loss::::0.587 : 0.15 : 0.035 Accuracy: 0.679 Best : 0.742\n",
      "5600-Accuracy: 0.991::::Loss::::0.558 : 0.137 : 0.069 Accuracy: 0.739 Best : 0.742\n",
      "5700-Accuracy: 0.984::::Loss::::0.619 : 0.146 : 0.055 Accuracy: 0.7 Best : 0.742\n",
      "5800-Accuracy: 0.991::::Loss::::0.572 : 0.134 : 0.053 Accuracy: 0.709 Best : 0.742\n",
      "5900-Accuracy: 0.991::::Loss::::0.509 : 0.131 : 0.04 Accuracy: 0.724 Best : 0.742\n",
      "6000-Accuracy: 0.994::::Loss::::0.639 : 0.119 : 0.027 Accuracy: 0.707 Best : 0.742\n",
      "6100-Accuracy: 0.989::::Loss::::0.596 : 0.13 : 0.047 Accuracy: 0.735 Best : 0.742\n",
      "6200-Accuracy: 0.988::::Loss::::0.58 : 0.129 : 0.047 Accuracy: 0.698 Best : 0.742\n",
      "6300-Accuracy: 0.99::::Loss::::0.561 : 0.126 : 0.045 Accuracy: 0.723 Best : 0.742\n",
      "6400-Accuracy: 0.994::::Loss::::0.591 : 0.134 : 0.042 Accuracy: 0.735 Best : 0.742\n",
      "6500-Accuracy: 0.992::::Loss::::0.567 : 0.136 : 0.045 Accuracy: 0.713 Best : 0.742\n",
      "6600-Accuracy: 0.996::::Loss::::0.588 : 0.134 : 0.049 Accuracy: 0.724 Best : 0.742\n",
      "6700-Accuracy: 0.989::::Loss::::0.605 : 0.135 : 0.026 Accuracy: 0.742 Best : 0.742\n",
      "6800-Accuracy: 0.989::::Loss::::0.596 : 0.134 : 0.022 Accuracy: 0.711 Best : 0.742\n",
      "6900-Accuracy: 0.989::::Loss::::0.616 : 0.136 : 0.043 Accuracy: 0.701 Best : 0.742\n",
      "7000-Accuracy: 0.988::::Loss::::0.58 : 0.145 : 0.044 Accuracy: 0.727 Best : 0.742\n",
      "7100-Accuracy: 0.993::::Loss::::0.579 : 0.127 : 0.061 Accuracy: 0.715 Best : 0.742\n",
      "7200-Accuracy: 0.996::::Loss::::0.584 : 0.129 : 0.038 Accuracy: 0.764 Best : 0.764\n",
      "7300-Accuracy: 0.998::::Loss::::0.575 : 0.125 : 0.017 Accuracy: 0.776 Best : 0.776\n",
      "7400-Accuracy: 0.989::::Loss::::0.612 : 0.136 : 0.038 Accuracy: 0.722 Best : 0.776\n",
      "7500-Accuracy: 0.992::::Loss::::0.62 : 0.141 : 0.038 Accuracy: 0.74 Best : 0.776\n",
      "7600-Accuracy: 0.991::::Loss::::0.558 : 0.146 : 0.051 Accuracy: 0.738 Best : 0.776\n",
      "7700-Accuracy: 0.993::::Loss::::0.559 : 0.139 : 0.031 Accuracy: 0.73 Best : 0.776\n",
      "7800-Accuracy: 0.994::::Loss::::0.6 : 0.136 : 0.025 Accuracy: 0.704 Best : 0.776\n",
      "7900-Accuracy: 0.992::::Loss::::0.606 : 0.142 : 0.025 Accuracy: 0.73 Best : 0.776\n",
      "8000-Accuracy: 0.994::::Loss::::0.643 : 0.131 : 0.016 Accuracy: 0.691 Best : 0.776\n",
      "8100-Accuracy: 0.994::::Loss::::0.57 : 0.138 : 0.038 Accuracy: 0.69 Best : 0.776\n",
      "8200-Accuracy: 0.989::::Loss::::0.623 : 0.13 : 0.044 Accuracy: 0.712 Best : 0.776\n",
      "8300-Accuracy: 0.993::::Loss::::0.601 : 0.126 : 0.026 Accuracy: 0.703 Best : 0.776\n",
      "8400-Accuracy: 0.994::::Loss::::0.57 : 0.14 : 0.017 Accuracy: 0.717 Best : 0.776\n",
      "8500-Accuracy: 0.996::::Loss::::0.648 : 0.143 : 0.016 Accuracy: 0.724 Best : 0.776\n",
      "8600-Accuracy: 0.993::::Loss::::0.595 : 0.14 : 0.02 Accuracy: 0.694 Best : 0.776\n",
      "8700-Accuracy: 0.991::::Loss::::0.615 : 0.136 : 0.032 Accuracy: 0.709 Best : 0.776\n",
      "8800-Accuracy: 0.991::::Loss::::0.603 : 0.134 : 0.025 Accuracy: 0.752 Best : 0.776\n",
      "8900-Accuracy: 0.992::::Loss::::0.597 : 0.134 : 0.018 Accuracy: 0.705 Best : 0.776\n",
      "9000-Accuracy: 0.994::::Loss::::0.569 : 0.132 : 0.026 Accuracy: 0.688 Best : 0.776\n",
      "9100-Accuracy: 0.997::::Loss::::0.603 : 0.128 : 0.024 Accuracy: 0.713 Best : 0.776\n",
      "9200-Accuracy: 0.994::::Loss::::0.588 : 0.153 : 0.024 Accuracy: 0.677 Best : 0.776\n",
      "9300-Accuracy: 0.993::::Loss::::0.599 : 0.139 : 0.02 Accuracy: 0.705 Best : 0.776\n",
      "9400-Accuracy: 0.993::::Loss::::0.633 : 0.145 : 0.012 Accuracy: 0.713 Best : 0.776\n",
      "9500-Accuracy: 0.995::::Loss::::0.619 : 0.134 : 0.014 Accuracy: 0.689 Best : 0.776\n",
      "9600-Accuracy: 0.995::::Loss::::0.524 : 0.135 : 0.018 Accuracy: 0.688 Best : 0.776\n",
      "9700-Accuracy: 0.996::::Loss::::0.588 : 0.133 : 0.029 Accuracy: 0.658 Best : 0.776\n",
      "9800-Accuracy: 0.992::::Loss::::0.575 : 0.126 : 0.032 Accuracy: 0.67 Best : 0.776\n",
      "9900-Accuracy: 0.994::::Loss::::0.606 : 0.131 : 0.027 Accuracy: 0.717 Best : 0.776\n"
     ]
    }
   ],
   "source": [
    "db = zsl_NShot(train_data,test_data, batchsz=10, n_way=args.n_way, k_shot=args.k_spt, k_query=args.k_qry)\n",
    "x_spt, y_spt, x_qry, y_qry = db.next('train')\n",
    "x_spt = torch.from_numpy(x_spt).cuda()\n",
    "x_qry = torch.from_numpy(x_qry).cuda()\n",
    "y_spt = torch.from_numpy(y_spt).cuda()\n",
    "y_qry = torch.from_numpy(y_qry).cuda()\n",
    "print('train :  ' +str(x_spt.shape))\n",
    "print('train_labels :  ' +str(y_spt.shape))\n",
    "print('test :  ' +str(x_qry.shape))\n",
    "print('test_labels :  ' +str(y_qry.shape))\n",
    "\n",
    "best=0\n",
    "maml = Meta(args).cuda()\n",
    "for itr in range(10000):\n",
    "    x_spt, y_spt, x_qry, y_qry = db.next('train')\n",
    "    x_spt = torch.from_numpy(x_spt).cuda()\n",
    "    x_qry = torch.from_numpy(x_qry).cuda()\n",
    "    y_spt = torch.from_numpy(y_spt).cuda()\n",
    "    y_qry = torch.from_numpy(y_qry).cuda()\n",
    "    \n",
    "    accs,loss_gen,loss_dis, loss_cla = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "    accs=np.mean(accs)\n",
    "    if (itr>=3000 and itr%100==0):\n",
    "        unseen_accuracy=test()\n",
    "        if unseen_accuracy>=best:\n",
    "            best=unseen_accuracy\n",
    "        print(str(itr)+'-Accuracy: '+str(round(accs,3))+'::::Loss::::'+str(round(loss_gen.item(),3))+ ' : ' +str(round(loss_dis.item(),3))+ ' : '+str(round(loss_cla.item(),3))+' Accuracy: '+str(round(unseen_accuracy,3))+' Best : '+str(round(best,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
