{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "rd=1234 #torch.randint(1,10000,[1])\n",
    "torch.manual_seed(rd)\n",
    "torch.cuda.manual_seed_all(rd)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: init dataset\n",
    "print(\"init dataset\")\n",
    "\n",
    "dataroot = '/data/dataset/zsl/data'\n",
    "dataset = 'CUB1_data'\n",
    "image_embedding = 'res101' \n",
    "class_embedding = 'att_splits'\n",
    "\n",
    "matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + image_embedding + \".mat\")\n",
    "feature = matcontent['features'].T\n",
    "label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + class_embedding + \".mat\")\n",
    "# numpy array index starts from 0, matlab starts from 1\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "\n",
    "attribute = matcontent['att'].T \n",
    "\n",
    "x = feature[trainval_loc] # train_features\n",
    "train_label = label[trainval_loc].astype(int)  # train_label\n",
    "att = attribute[train_label] # train attributes\n",
    "\n",
    "x_test = feature[test_unseen_loc]  # test_feature\n",
    "test_label = label[test_unseen_loc].astype(int) # test_label\n",
    "x_test_seen = feature[test_seen_loc]  #test_seen_feature\n",
    "test_label_seen = label[test_seen_loc].astype(int) # test_seen_label\n",
    "test_id = np.unique(test_label)   # test_id\n",
    "att_pro = attribute[test_id]      # test_attribute\n",
    "\n",
    "import numpy as np\n",
    "path=dataroot\n",
    "file=path+'/cub_attributes_reed.npy'\n",
    "attribute=15*np.load(file)\n",
    "\n",
    "\n",
    "# train set\n",
    "#train_features=torch.from_numpy(x)\n",
    "train_features=x\n",
    "print('train_features.shape: ' + str(train_features.shape))\n",
    "\n",
    "train_label=np.array(torch.from_numpy(train_label).unsqueeze(1))\n",
    "#train_label=torch.from_numpy(train_label).unsqueeze(1)\n",
    "print('train_label.shape:  '+str(train_label.shape))\n",
    "\n",
    "# attributes\n",
    "all_attributes=np.array(attribute)\n",
    "print('all_attributes.shape:  '+str(all_attributes.shape))\n",
    "\n",
    "attributes = torch.from_numpy(attribute)\n",
    "# test set\n",
    "\n",
    "# test_features=torch.from_numpy(x_test)\n",
    "test_features=x_test\n",
    "print('test_features.shape:  '+ str(test_features.shape))\n",
    "\n",
    "test_label=np.array(torch.from_numpy(test_label).unsqueeze(1))\n",
    "print('test_label.shape:  ' +str(test_label.shape))\n",
    "\n",
    "testclasses_id = np.array(test_id)\n",
    "print('testclasses_id.shape:  ' +str(testclasses_id.shape))\n",
    "\n",
    "test_attributes = torch.from_numpy(att_pro).float()\n",
    "print('test_attributes.shape:  ' +str(test_attributes.shape))\n",
    "\n",
    "\n",
    "test_seen_features = torch.from_numpy(x_test_seen)\n",
    "print('test_seen_features.shape:  ' +str(test_seen_features.shape))\n",
    "\n",
    "test_seen_label = torch.from_numpy(test_label_seen)\n",
    "\n",
    "train_data = [train_features,train_label]\n",
    "test_data = [test_features,test_label]\n",
    "\n",
    "unq_train_labels=np.unique(train_label)\n",
    "unq_test_labels=np.unique(test_label)\n",
    "#train_data = TensorDataset(train_features,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight, mean=0, std=0.002)\n",
    "        m.bias.data.fill_(0.002)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.modelG = nn.Sequential(\n",
    "            nn.Linear(args.attri_dim+args.noise_dim, 2048),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048, 0.8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048, 0.8),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, args.input_shape)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, noise, attri,weightsM=None):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((attri,noise), -1)\n",
    "        if weightsM is None:\n",
    "            img = self.modelG(gen_input)\n",
    "        else:\n",
    "            i=0\n",
    "            weights=weightsM[0]\n",
    "            for m in self.modelG.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "                if isinstance(m,nn.BatchNorm1d):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "            img = self.modelG(gen_input)\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.modelD = nn.Sequential(\n",
    "            nn.Linear(args.input_shape+args.attri_dim, 2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "#         self.modelD.apply(init_weights)\n",
    "\n",
    "    def forward(self, img, attri,weightsM=None):\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), attri), -1)\n",
    "        if weightsM is None:\n",
    "            real_fake = self.modelD(d_in)\n",
    "        else:\n",
    "            i=0\n",
    "            weights=weightsM[1]\n",
    "            for m in self.modelD.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "                if isinstance(m,nn.BatchNorm1d):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "            real_fake = self.modelD(d_in)\n",
    "        \n",
    "        return real_fake\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.modelC = nn.Sequential(\n",
    "            nn.Linear(args.input_shape, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, args.num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_gen, target,weightsM=None):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        if weightsM is None:\n",
    "            output = self.modelC(img_gen)\n",
    "        else:\n",
    "            i=0\n",
    "            weights=weightsM[2]\n",
    "            for m in self.modelC.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "                if isinstance(m,nn.BatchNorm1d):\n",
    "                    m.weight.data=weights[i]\n",
    "                    m.bias.data=weights[i+1]\n",
    "                    i=i+2\n",
    "            output = self.modelC(img_gen)\n",
    "        return output\n",
    "\n",
    "class Classifier2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier2, self).__init__()\n",
    "\n",
    "        self.modelC2 = nn.Sequential(\n",
    "            nn.Linear(args.input_shape, 1024),\n",
    "            nn.BatchNorm1d(1024, 0.8),\n",
    "            nn.Linear(1024, args.num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_gen, target,weightsM=None):\n",
    "        output2 = self.modelC2(img_gen)\n",
    "        return output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testclasses_id)\n",
    "class_weight=torch.zeros(attribute.shape[0])\n",
    "\n",
    "for i in range(attribute.shape[0]):\n",
    "    if (i in testclasses_id):\n",
    "        class_weight[i]=1.0\n",
    "    else:\n",
    "        class_weight[i]=0.1\n",
    "# print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class all_arguments():\n",
    "    n_way=10\n",
    "    k_spt=5\n",
    "    k_qry=3\n",
    "    \n",
    "    imgsz=2048\n",
    "    sigma_ts=0.25\n",
    "    sigma_tr=0.5\n",
    "    \n",
    "    meta_lr=1e-5\n",
    "    meta_lrD=1e-3\n",
    "    update_lr=1e-3\n",
    "    update_step=5\n",
    "\n",
    "    input_shape=2048\n",
    "    num_class=200\n",
    "    attri_dim=1024\n",
    "    noise_dim=512\n",
    "    clssifier_weight=0.05\n",
    "\n",
    "    # Get options\n",
    "    attributes=attributes\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "    \n",
    "args=all_arguments()\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "classifier=Classifier()\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    classifier.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "    \n",
    "para=list(classifier.parameters())\n",
    "#print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "from    torch.utils.data import TensorDataset, DataLoader\n",
    "from    torch import optim\n",
    "import  numpy as np\n",
    "\n",
    "glen=len(list(generator.parameters()))\n",
    "gen_para=list(generator.parameters())+ list(classifier.parameters())\n",
    "\n",
    "disc_optim = optim.SGD(discriminator.parameters(), lr=args.meta_lrD,weight_decay=1e-6)\n",
    "gen_optim = optim.Adam(gen_para, lr=args.meta_lr,betas=(0.9, 0.99),weight_decay=1e-6)\n",
    "\n",
    "disc_schedular = StepLR(disc_optim,step_size=100,gamma=0.95)\n",
    "gen_schedular = StepLR(gen_optim,step_size=100,gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = args.update_lr\n",
    "        self.meta_lr = args.meta_lr\n",
    "        self.n_way = args.n_way\n",
    "        self.k_spt = args.k_spt\n",
    "        self.k_qry = args.k_qry\n",
    "        self.cls_weight=args.clssifier_weight\n",
    "        self.update_step = args.update_step\n",
    "        cuda=args.cuda\n",
    "        self.noise_dim=args.noise_dim\n",
    "        #self.all_loss=all_loss()\n",
    "        \n",
    "        self.FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "        self.glen=glen\n",
    "        self.gen_para=gen_para\n",
    "        self.disc_optim = disc_optim\n",
    "        self.gen_optim = gen_optim\n",
    "\n",
    "        self.disc_schedular = disc_schedular\n",
    "        self.gen_schedular = gen_schedular\n",
    "\n",
    "    def clip_grad_by_norm_(self, grad, max_norm):\n",
    "        \"\"\"\n",
    "        in-place gradient clipping.\n",
    "        :param grad: list of gradients\n",
    "        :param max_norm: maximum norm allowable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        total_norm = 0\n",
    "        counter = 0\n",
    "        for g in grad:\n",
    "            param_norm = g.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            counter += 1\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            for g in grad:\n",
    "                g.data.mul_(clip_coef)\n",
    "\n",
    "        return total_norm/counter\n",
    "    \n",
    "    def all_loss(self, img_feature,img_labels,fast_weight=None):\n",
    "        batch_size = img_feature.shape[0]\n",
    "        FloatTensor=self.FloatTensor\n",
    "        LongTensor=self.LongTensor\n",
    "        img_labels=img_labels.type(LongTensor)\n",
    "\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(img_feature.type(FloatTensor))\n",
    "        gen_attri = Variable(attributes[img_labels].type(FloatTensor))\n",
    "        \n",
    "        z = Variable(FloatTensor(np.random.normal(0,args.sigma_tr, (batch_size, self.noise_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_attri,fast_weight)\n",
    "        validity = discriminator(gen_imgs, gen_attri,fast_weight)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        #print('g_loss: '+str(g_loss))\n",
    "        \n",
    "        \n",
    "        validity_real = discriminator(real_imgs, gen_attri,fast_weight)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "        #print('d_real_loss: '+str(d_real_loss))\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_attri,fast_weight)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "        #print('d_fake_loss: '+str(d_fake_loss))\n",
    "\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        \n",
    "        cls_out=classifier(gen_imgs,img_labels,fast_weight)\n",
    "        cls_out=F.log_softmax(cls_out, dim=1)\n",
    "        c_loss = F.nll_loss(cls_out, img_labels)\n",
    "        \n",
    "        \n",
    "        return g_loss, d_loss, c_loss, cls_out\n",
    "    \n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "        :param x_spt:   [b, setsz, d]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, d]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        task_num, setsz,d = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        losses_gen = [0 for _ in range(self.update_step + 1)]  # losses_q[i], i is tasks idx\n",
    "        losses_dis = [0 for _ in range(self.update_step + 1)]\n",
    "        losses_cla = [0 for _ in range(self.update_step + 1)]\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i])\n",
    "             \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(y_spt[i].view_as(pred)).sum().item()\n",
    "            \n",
    "            \n",
    "            joint_gen_loss=g_loss+self.cls_weight*c_loss\n",
    "                                      \n",
    "            gen_grad = torch.autograd.grad(joint_gen_loss, self.gen_para)\n",
    "            fast_weights_G = list(map(lambda g: g[1] - self.update_lr * g[0], zip(gen_grad, self.gen_para)))\n",
    "            \n",
    "            disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "            fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "\n",
    "#             clas_grad = torch.autograd.grad(c_loss, self.discriminator.parameters())\n",
    "#             fast_weights_C = list(map(lambda c: c[1] - self.update_lr * c[0], zip(clas_grad, self.classifier.parameters())))\n",
    "            fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i])\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                losses_gen[0] += g_loss\n",
    "                losses_dis[0] += d_loss\n",
    "                losses_cla[0] += c_loss\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # how to initialise with the fast weight\n",
    "                losses_gen[1] += g_loss\n",
    "                losses_dis[1] += d_loss\n",
    "                losses_cla[1] += c_loss\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i],fast_weight)\n",
    "                joint_gen_loss=g_loss+self.cls_weight*c_loss\n",
    "\n",
    "                gen_grad = torch.autograd.grad(joint_gen_loss, self.gen_para)\n",
    "                fast_weights_G = list(map(lambda g: g[1] - self.update_lr * g[0], zip(gen_grad, self.gen_para)))\n",
    "\n",
    "                disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "                fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "                \n",
    "                fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "                # 2. compute grad on theta_pi\n",
    "                \n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step. \n",
    "                losses_gen[k+1] += g_loss\n",
    "                losses_dis[k+1] += d_loss\n",
    "                losses_cla[k+1] += c_loss\n",
    "                corrects[k+1] = corrects[k+1] + correct\n",
    "                \n",
    "                ################################## Unseen class test #####################################\n",
    "\n",
    "            # 4. record last step's loss for task i\n",
    "            losses_gen.append(g_loss)\n",
    "            losses_dis.append(d_loss)\n",
    "            losses_cla.append(c_loss)\n",
    "                                      \n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_gen=0\n",
    "        for itr in range(task_num):\n",
    "            loss_gen+=losses_gen[-task_num:][itr]\n",
    "        loss_gen=loss_gen/task_num\n",
    "        \n",
    "        loss_dis=0\n",
    "        for itr in range(task_num):\n",
    "            loss_dis+=losses_dis[-task_num:][itr]\n",
    "        loss_dis=loss_dis/task_num\n",
    "        \n",
    "        loss_cla=0\n",
    "        for itr in range(task_num):\n",
    "            loss_cla+=losses_cla[-task_num:][itr]\n",
    "        loss_cla=loss_cla/task_num\n",
    "            \n",
    "        joint_Gloss=loss_gen+self.cls_weight*loss_cla\n",
    "        \n",
    "        # optimize theta parameters\n",
    "        self.disc_optim.zero_grad()\n",
    "        self.gen_optim.zero_grad()\n",
    "        \n",
    "        joint_Gloss.backward()\n",
    "        loss_dis.backward()\n",
    "        \n",
    "        self.disc_optim.step()\n",
    "        self.gen_optim.step()\n",
    "        \n",
    "        \n",
    "        ######################################################################################\n",
    "        losses_gen = [0 for _ in range(self.update_step + 1)]  # losses_q[i], i is tasks idx\n",
    "        losses_dis = [0 for _ in range(self.update_step + 1)]\n",
    "        losses_cla = [0 for _ in range(self.update_step + 1)]\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i])\n",
    "             \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(y_spt[i].view_as(pred)).sum().item()\n",
    "            \n",
    "            disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "            fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "\n",
    "#             clas_grad = torch.autograd.grad(c_loss, self.discriminator.parameters())\n",
    "#             fast_weights_C = list(map(lambda c: c[1] - self.update_lr * c[0], zip(clas_grad, self.classifier.parameters())))\n",
    "            fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i])\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                losses_gen[0] += g_loss\n",
    "                losses_dis[0] += d_loss\n",
    "                losses_cla[0] += c_loss\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # how to initialise with the fast weight\n",
    "                losses_gen[1] += g_loss\n",
    "                losses_dis[1] += d_loss\n",
    "                losses_cla[1] += c_loss\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_spt[i],y_spt[i],fast_weight)\n",
    "\n",
    "                disc_grad = torch.autograd.grad(d_loss, discriminator.parameters())\n",
    "                fast_weights_D = list(map(lambda d: d[1] - self.update_lr * d[0], zip(disc_grad, discriminator.parameters())))\n",
    "                \n",
    "                fast_weight=[fast_weights_G[0:self.glen],fast_weights_D,fast_weights_G[self.glen:]]\n",
    "                # 2. compute grad on theta_pi\n",
    "                \n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                g_loss,d_loss,c_loss,output=self.all_loss(x_qry[i],y_qry[i],fast_weight)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(y_qry[i].view_as(pred)).sum().item()\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step. \n",
    "                losses_gen[k+1] += g_loss\n",
    "                losses_dis[k+1] += d_loss\n",
    "                losses_cla[k+1] += c_loss\n",
    "                corrects[k+1] = corrects[k+1] + correct\n",
    "                \n",
    "                ################################## Unseen class test #####################################\n",
    "\n",
    "            # 4. record last step's loss for task i\n",
    "            losses_gen.append(g_loss)\n",
    "            losses_dis.append(d_loss)\n",
    "            losses_cla.append(c_loss)\n",
    "                                      \n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        \n",
    "        loss_dis=0\n",
    "        for itr in range(task_num):\n",
    "            loss_dis+=losses_dis[-task_num:][itr]\n",
    "        loss_dis=loss_dis/task_num\n",
    "        \n",
    "        # optimize theta parameters\n",
    "        self.disc_optim.zero_grad()\n",
    "        loss_dis.backward()\n",
    "        \n",
    "        self.disc_optim.step()\n",
    "\n",
    "        ######################################################################################\n",
    "        \n",
    "        accs = np.array(corrects) / (querysz * task_num)\n",
    "        #all_loss=[losses_gen,losses_dis,losses_cla]\n",
    "\n",
    "        return accs, loss_gen,loss_dis, loss_cla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torchvision.transforms as transforms\n",
    "from    PIL import Image\n",
    "import  os.path\n",
    "import  numpy as np\n",
    "\n",
    "\n",
    "class zsl_NShot:\n",
    "\n",
    "    def __init__(self, trainData,testData, batchsz, n_way, k_shot, k_query):\n",
    "        \"\"\"\n",
    "        Different from mnistNShot, the\n",
    "        :param root:\n",
    "        :param batchsz: task num\n",
    "        :param n_way:\n",
    "        :param k_shot:\n",
    "        :param k_qry:\n",
    "        :param imgsz:\n",
    "        \"\"\"\n",
    "        self.x_train, self.x_test = trainData, testData  \n",
    "        # self.normalization()\n",
    "        self.batchsz = batchsz # number of task\n",
    "        self.n_way = n_way  # n way\n",
    "        self.k_shot = k_shot  # k shot\n",
    "        self.k_query = k_query  # k query\n",
    "\n",
    "        # save pointer of current read batch in total cache\n",
    "        self.indexes = {\"train\": 0, \"test\": 0}\n",
    "        self.datasets = {\"train\": self.x_train, \"test\": self.x_test}  # original data cached\n",
    "        #print(\"DB: train\", self.x_train[0].shape, \"test\", self.x_test[0].shape)\n",
    "\n",
    "        self.datasets_cache = {\"train\": self.load_data_cache(self.datasets[\"train\"])} # current epoch data cached\n",
    "                              # \"test\": self.load_data_cache(self.datasets[\"test\"])}\n",
    "\n",
    "    def normalization(self):\n",
    "        \"\"\"\n",
    "        Normalizes our data, to have a mean of 0 and sdt of 1\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "        # print(\"before norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "        self.x_train = (self.x_train - self.mean) / self.std\n",
    "        self.x_test = (self.x_test - self.mean) / self.std\n",
    "\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "\n",
    "    # print(\"after norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "\n",
    "    def load_data_cache(self, data_pack):\n",
    "        \"\"\"\n",
    "        Collects several batches data for N-shot learning\n",
    "        :param data_pack: [N,2048]\n",
    "        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "        \"\"\"\n",
    "        #  take 5 way 1 shot as example: 5 * 1\n",
    "        setsz = self.k_shot * self.n_way\n",
    "        querysz = self.k_query * self.n_way\n",
    "        data_cache = []\n",
    "\n",
    "        unq_labels=np.unique(data_pack[1])\n",
    "        labels=np.array(data_pack[1])\n",
    "        \n",
    "        Data=data_pack[0]\n",
    "       \n",
    "        x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
    "        for i in range(self.batchsz):  # one batch means one set\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
    "#             print('unq labels shape: '+str(unq_labels.shape)+'  ' +str(i))\n",
    "            selected_cls = np.random.choice(unq_labels, 2*self.n_way, False)\n",
    "#             print(unq_labels)\n",
    "#                 print('selected: '+str(selected_cls))\n",
    "\n",
    "            for j, cur_class in enumerate(selected_cls[:self.n_way]):\n",
    "                indx=np.where(labels==cur_class)[0]\n",
    "                selected_img = np.random.choice(indx, self.k_shot, False)\n",
    "                x_spt.append(Data[selected_img])\n",
    "                y_spt.append(labels[selected_img])\n",
    "                #print('shape: '+str(np.array(x_spt).shape))\n",
    "\n",
    "            for j, cur_class in enumerate(selected_cls[self.n_way:]):\n",
    "                indx=np.where(labels==cur_class)[0]\n",
    "                selected_img = np.random.choice(indx, self.k_query, False)\n",
    "                x_qry.append(Data[selected_img])\n",
    "                y_qry.append(labels[selected_img])\n",
    "                #print('shape: '+str(np.array(x_spt).shape))\n",
    "\n",
    "            # shuffle inside a batch\n",
    "            perm = np.random.permutation(self.n_way * self.k_shot)\n",
    "            x_spt = np.array(x_spt).reshape(self.n_way * self.k_shot, 2048)[perm]\n",
    "            y_spt = np.array(y_spt).reshape(self.n_way * self.k_shot)[perm]\n",
    "            perm = np.random.permutation(self.n_way * self.k_query)\n",
    "            x_qry = np.array(x_qry).reshape(self.n_way * self.k_query, 2048)[perm]\n",
    "            y_qry = np.array(y_qry).reshape(self.n_way * self.k_query)[perm]\n",
    "\n",
    "            # append [N,2048] => [b, N,2048]\n",
    "            x_spts.append(x_spt)\n",
    "            y_spts.append(y_spt)\n",
    "            x_qrys.append(x_qry)\n",
    "            y_qrys.append(y_qry)\n",
    "\n",
    "\n",
    "        # [b, N,2048]\n",
    "        x_spts = np.array(x_spts).astype(np.float32).reshape(self.batchsz, setsz, 2048)\n",
    "        y_spts = np.array(y_spts).astype(np.int).reshape(self.batchsz, setsz)\n",
    "        # [b, N,2048]\n",
    "        x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 2048)\n",
    "        y_qrys = np.array(y_qrys).astype(np.int).reshape(self.batchsz, querysz)\n",
    "\n",
    "        data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "\n",
    "        return data_cache\n",
    "\n",
    "    def next(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Gets next batch from the dataset with name.\n",
    "        :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # update cache if indexes is larger cached num\n",
    "        if self.indexes[mode] >= len(self.datasets_cache[mode]):\n",
    "            self.indexes[mode] = 0\n",
    "            self.datasets_cache[mode] = self.load_data_cache(self.datasets[mode])\n",
    "\n",
    "        next_batch = self.datasets_cache[mode][self.indexes[mode]]\n",
    "        self.indexes[mode] += 1\n",
    "\n",
    "        return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "def accuracy_zsl(gen_imgs,img_labels,testData,testLabels):\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    gen_imgs=(torch.from_numpy(gen_imgs)).type(FloatTensor)\n",
    "    img_labels=(torch.from_numpy(img_labels)).type(LongTensor)\n",
    "    testData=(torch.from_numpy(testData)).type(FloatTensor)\n",
    "    testLabels=(torch.from_numpy(testLabels)).type(LongTensor)\n",
    "    \n",
    "    ########################################################\n",
    "    # For the training of the classifier, we are using synthesize sample for both seen and unseen class,\n",
    "    # to overcome the bias towards the seen classes.\n",
    "    # But at the same time we can use seen class training data also with the synthesize seen and unseen\n",
    "    # class data. If you want to use this uncomment the below 4 lines. It gives the GZSL H-mean ~60. \n",
    "    # While in the paper we use former case and have H-mean ~56.\n",
    "    \n",
    "#     train_features=(torch.from_numpy(x)).type(FloatTensor) \n",
    "#     train_label1=(torch.from_numpy(train_label).squeeze(1)).type(LongTensor)    \n",
    "#     gen_imgs=torch.cat([gen_imgs,train_features])\n",
    "#     img_labels=torch.cat([img_labels,train_label1])\n",
    "\n",
    "    train_data = TensorDataset(gen_imgs,img_labels)\n",
    "    train_loader = DataLoader(train_data,batch_size=32,shuffle=True)\n",
    "    \n",
    "    \n",
    "    test_seen_features1=test_seen_features.type(FloatTensor)\n",
    "    test_seen_label1=test_seen_label.type(LongTensor)\n",
    "        \n",
    "    classifier2=Classifier2()\n",
    "    classifier2.cuda()\n",
    "    \n",
    "    cls2_para=list(p for p in classifier2.parameters() if p.requires_grad)\n",
    "    cls2_optim = optim.Adam(cls2_para, lr=args.meta_lrD,betas=(0.9, 0.999),weight_decay=1e-3)\n",
    "    cls2_schedular=StepLR(cls2_optim,step_size=100,gamma=0.9)\n",
    "    \n",
    "    besthmean=0\n",
    "    epsln=0.001\n",
    "    US=[]\n",
    "    for it in range(8001):\n",
    "        cls2_schedular.step()\n",
    "        # optimize theta parameters\n",
    "        cls2_optim.zero_grad()\n",
    "        gen_imgs,img_labels = train_loader.__iter__().next()\n",
    "        \n",
    "        cls_out=classifier2(gen_imgs,img_labels)\n",
    "        c_loss2=auxiliary_loss(cls_out, img_labels)\n",
    "\n",
    "        c_loss2.backward()\n",
    "        cls2_optim.step()\n",
    "        \n",
    "        if it%10==0:\n",
    "            with torch.no_grad():\n",
    "                cls_out=classifier2(testData,testLabels)\n",
    "                pred = cls_out.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(testLabels.view_as(pred)).sum().item()\n",
    "                ACC_unseen=correct/testLabels.shape[0] +epsln\n",
    "                \n",
    "                cls_out=classifier2(test_seen_features1,test_seen_label1)\n",
    "                pred = cls_out.max(1, keepdim=True)[1]\n",
    "                correct = pred.eq(test_seen_label1.view_as(pred)).sum().item()\n",
    "                ACC_seen=correct/test_seen_label1.shape[0]+epsln\n",
    "                H_mean=(2*ACC_unseen*ACC_seen)/(ACC_unseen+ACC_seen)\n",
    "        \n",
    "            if H_mean>besthmean:\n",
    "                besthmean=H_mean\n",
    "                US=[ACC_unseen,ACC_seen]\n",
    "\n",
    "            \n",
    "        if (it%2000)==0:\n",
    "            print('curr_Hmean: '+str(round(H_mean,3))+ '  Best-Hmean: '+str(round(besthmean,3)) + '  US  '+str(US))\n",
    "        \n",
    "    del classifier2\n",
    "    return H_mean,besthmean\n",
    "            \n",
    "\n",
    "\n",
    "def test_zsl(genunseen_input,test_labels_repeat,test_features,testLabels):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            gen_imgs = generator(genunseen_input[:,args.attri_dim:], genunseen_input[:,:args.attri_dim])\n",
    "            \n",
    "            pseudoTrainData = np.array(gen_imgs.cpu())\n",
    "            testData = test_features\n",
    "            pseudoTrainLabels=test_labels_repeat\n",
    "            testLabels=testLabels\n",
    "            \n",
    "        hmean,besthmean=accuracy_zsl(pseudoTrainData,pseudoTrainLabels,testData,testLabels)\n",
    "            \n",
    "\n",
    "        return hmean,besthmean\n",
    "\n",
    "def test():\n",
    "    # Generate seen class samples\n",
    "    testunq_labels=np.unique(test_label)\n",
    "    Nsample=100\n",
    "    test_attri_repeat=[]\n",
    "    test_labels_repeat=[]\n",
    "    for i in testunq_labels:\n",
    "        lab_attribute=np.reshape(attribute[i],[1,args.attri_dim])\n",
    "        test_attri_repeat.append(np.repeat(lab_attribute,Nsample,axis=0))\n",
    "        test_labels_repeat.append(np.repeat(i,Nsample,axis=0))\n",
    "    test_attri_repeat=np.concatenate(test_attri_repeat,0)\n",
    "\n",
    "    \n",
    "    test_labels_repeat=np.concatenate(test_labels_repeat,0)\n",
    "    z = np.random.normal(0, args.sigma_ts, (Nsample*testunq_labels.shape[0], args.noise_dim))\n",
    "    genunseen_input = np.concatenate((test_attri_repeat,z), 1)\n",
    "\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    genunseen_input=(torch.from_numpy(genunseen_input)).type(FloatTensor)\n",
    "#     print(genunseen_input.shape)\n",
    "#     print(test_labels_repeat.shape)\n",
    "    \n",
    "    # Generate Unseen class samples\n",
    "    # unq_train_labels\n",
    "    Nsample=100\n",
    "    train_attri_repeat=[]\n",
    "    train_labels_repeat=[]\n",
    "    for i in unq_train_labels:\n",
    "        lab_attribute=np.reshape(attribute[i],[1,args.attri_dim])\n",
    "        train_attri_repeat.append(np.repeat(lab_attribute,Nsample,axis=0))\n",
    "        train_labels_repeat.append(np.repeat(i,Nsample,axis=0))\n",
    "    train_attri_repeat=np.concatenate(train_attri_repeat,0)\n",
    "\n",
    "    train_labels_repeat=np.concatenate(train_labels_repeat,0)\n",
    "    z = np.random.normal(0, args.sigma_ts, (Nsample*unq_train_labels.shape[0], args.noise_dim))\n",
    "    genseen_input = np.concatenate((train_attri_repeat,z), 1)\n",
    "\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    genseen_input=(torch.from_numpy(genseen_input)).type(FloatTensor)\n",
    "    \n",
    "#     print(genseen_input.shape)\n",
    "#     print(train_labels_repeat.shape)\n",
    "    \n",
    "    seen_unseen_input=torch.cat([genunseen_input,genseen_input])\n",
    "    seen_unseen_labels=np.concatenate([test_labels_repeat,train_labels_repeat])\n",
    "\n",
    "#     print(seen_unseen_input.shape)\n",
    "#     print(seen_unseen_labels.shape)\n",
    "    \n",
    "    hmean,besthmean=test_zsl(seen_unseen_input,seen_unseen_labels,test_features,test_label)\n",
    "    return hmean,besthmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db = zsl_NShot(train_data,test_data, batchsz=10, n_way=args.n_way, k_shot=args.k_spt, k_query=args.k_qry)\n",
    "x_spt, y_spt, x_qry, y_qry = db.next('train')\n",
    "x_spt = torch.from_numpy(x_spt).cuda()\n",
    "x_qry = torch.from_numpy(x_qry).cuda()\n",
    "y_spt = torch.from_numpy(y_spt).cuda()\n",
    "y_qry = torch.from_numpy(y_qry).cuda()\n",
    "print('train :  ' +str(x_spt.shape))\n",
    "print('train_labels :  ' +str(y_spt.shape))\n",
    "print('test :  ' +str(x_qry.shape))\n",
    "print('test_labels :  ' +str(y_qry.shape))\n",
    "\n",
    "best=0\n",
    "bestsoft=0\n",
    "maml = Meta(args).cuda()\n",
    "for itr in range(10000):\n",
    "    x_spt, y_spt, x_qry, y_qry = db.next('train')\n",
    "    x_spt = torch.from_numpy(x_spt).cuda()\n",
    "    x_qry = torch.from_numpy(x_qry).cuda()\n",
    "    y_spt = torch.from_numpy(y_spt).cuda()\n",
    "    y_qry = torch.from_numpy(y_qry).cuda()\n",
    "    \n",
    "    accs,loss_gen,loss_dis, loss_cla = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "    accs=np.mean(accs)\n",
    "    if itr%100==0:\n",
    "        hmean,besthmean=test()\n",
    "#         if soft_accuracy>=bestsoft:\n",
    "#             bestsoft=soft_accuracy\n",
    "#         print('accuracy: ' +str(soft_accuracy)+'  Best: '+str(bestsoft))\n",
    "            \n",
    "        if besthmean>=best:\n",
    "            best=besthmean\n",
    "            \n",
    "#             torch.save(generator,'./generator.pth')\n",
    "#             torch.save(discriminator,'./discriminator.pth')\n",
    "#             torch.save(classifier,'./classifier.pth')\n",
    "#             torch.save(classifier2,'./classifier2.pth')\n",
    "            \n",
    "        print(str(itr)+'-Accuracy: '+str(round(accs,3))+'::::Loss::::'+str(round(loss_gen.item(),3))+ ' : ' +str(round(loss_dis.item(),3))+ ' : '+str(round(loss_cla.item(),3))+' Accuracy: '+str(round(hmean,3))+' Best : '+str(round(best,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
